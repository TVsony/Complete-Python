{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f16da5-0afa-4067-a8f5-48b239b0a8cd",
   "metadata": {},
   "source": [
    "## 1. What is the process for loading a dataset from an external source?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a466d-bce9-4ed8-9b16-2974f9f91966",
   "metadata": {},
   "source": [
    "**Loading Data from Local CSV Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76460dd-b519-4849-ae65-96f5169bf612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date  AveragePrice  Total Volume     4046      4225  \\\n",
       "0           0  2015-12-27          1.33      64236.62  1036.74  54454.85   \n",
       "1           1  2015-12-20          1.35      54876.98   674.28  44638.81   \n",
       "\n",
       "    4770  Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0  48.16     8696.87     8603.62       93.25          0.0  conventional  2015   \n",
       "1  58.33     9505.56     9408.07       97.49          0.0  conventional  2015   \n",
       "\n",
       "   region  \n",
       "0  Albany  \n",
       "1  Albany  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f675b-6726-45c2-bf57-c6cf9bb7c4af",
   "metadata": {},
   "source": [
    "**Excel Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01f41dc-c0cc-470a-8889-b20dad64e214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.8</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex   bmi  children smoker     region  expenses\n",
       "0   19  female  27.9         0    yes  southwest  16884.92\n",
       "1   18    male  33.8         1     no  southeast   1725.55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('insurance.xlsx')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3abac9-6fd9-4238-a429-7a1d04da8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.read_json('path/to/your/file.json')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df = pd.read_json('path/to/your/file.json')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ca617c-5c27-4c7d-9044-47061b6bbd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"url = 'http://example.com/data.csv'\\ndf = pd.read_csv(url)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"url = 'http://example.com/data.csv'\n",
    "df = pd.read_csv(url)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fca526f-7391-4cfc-985a-824997761a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport sqlite3\\n# Create a connection to the database\\nconn = sqlite3.connect('path/to/your/database.db')\\n\\n# Load data into a DataFrame\\ndf = pd.read_sql_query('SELECT * FROM your_table', conn)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sqlite3\n",
    "# Create a connection to the database\n",
    "conn = sqlite3.connect('path/to/your/database.db')\n",
    "\n",
    "# Load data into a DataFrame\n",
    "df = pd.read_sql_query('SELECT * FROM your_table', conn)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9d89d-e613-4de3-a955-dce50168f638",
   "metadata": {},
   "source": [
    "**Loading Data from Web APIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9345d796-1141-46b0-a82a-b12f0df5be22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport requests\\n# Make a GET request to the API\\nresponse = requests.get('http://api.example.com/data')\\n# Convert the response to JSON\\ndata = response.json()\\n# Load data into a DataFrame\\ndf = pd.DataFrame(data)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "# Make a GET request to the API\n",
    "response = requests.get('http://api.example.com/data')\n",
    "# Convert the response to JSON\n",
    "data = response.json()\n",
    "# Load data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc66b2-a2db-48a8-b749-7ff142bf3407",
   "metadata": {},
   "source": [
    "## 2. How can we use pandas to read JSON files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd0e353-b022-4f62-8b2b-0117819602ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\ndf = pd.read_json('file path .json')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "df = pd.read_json('file path .json')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d825bb-36f3-432d-b51e-836831ee7798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nurl = 'http://example.com/data.json'\\ndf = pd.read_json(url)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read JSON from a URL:\n",
    "'''\n",
    "url = 'http://example.com/data.json'\n",
    "df = pd.read_json(url)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec9620-d276-4d9e-ba6f-965846ba73e4",
   "metadata": {},
   "source": [
    "## 3. Describe the significance of DASK.\n",
    "\n",
    "Dask is a Python library that provides parallel computing and task scheduling functionality, primarily designed to handle large datasets that don't fit into memory. Its significance lies in several key areas:\n",
    "\n",
    "Scalability: Dask enables scalable computing by breaking down large datasets into smaller tasks, which can then be distributed across multiple cores in a single machine or across a cluster of machines. This allows for efficient utilization of computational resources, enabling users to tackle datasets that would otherwise be too large to process on a single machine.\n",
    "\n",
    "Parallelism: With Dask, computations can be parallelized easily, leading to faster processing times for tasks that can be broken down into smaller, independent units of work. This parallelism can be leveraged not only for CPU-bound tasks but also for I/O-bound operations, such as reading and writing large files.\n",
    "\n",
    "Integration with Existing Libraries: Dask seamlessly integrates with popular Python libraries such as NumPy, Pandas, and scikit-learn, allowing users to leverage their familiar APIs while taking advantage of Dask's parallel and distributed computing capabilities. This makes it easier for data scientists and analysts to scale their existing workflows to handle larger datasets.\n",
    "\n",
    "Flexibility: Dask provides a flexible programming interface that allows users to express complex computations using familiar Python syntax. It offers both high-level collections (such as Dask arrays and dataframes) for easy parallelization of common data manipulation tasks and low-level task scheduling primitives for fine-grained control over computation execution.\n",
    "\n",
    "Distributed Computing: Dask supports distributed computing across multiple machines using a scheduler-worker architecture. This enables users to scale their computations to clusters of machines without needing to rewrite their code, making it suitable for processing extremely large datasets in distributed environments such as Hadoop or cloud platforms.\n",
    "\n",
    "Overall, Dask is significant for its ability to democratize big data processing in Python, providing a powerful and flexible tool for analyzing large datasets efficiently and easily scaling computations to handle big data challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f55af8-3012-4274-a1f1-e74d657bec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "# Create a large array using Dask\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "# Perform some computation (e.g., sum of elements) in parallel\n",
    "result = x.sum()\n",
    "# Compute the result\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53103b-b603-4eb8-a0ac-2783a6301b11",
   "metadata": {},
   "source": [
    "## 4. Describe the functions of DASK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5ff382-b003-4abb-a13b-cddb62c06f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of array elements: 49999337.79198763\n",
      "Result of task scheduling with Dask: 40\n",
      "Mean of column 'x': 0.500134502451156\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parallel computing with Dask arrays\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "result = x.sum()\n",
    "print(\"Sum of array elements:\", result.compute())\n",
    "\n",
    "# Task scheduling with Dask\n",
    "@dask.delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Define a computation graph\n",
    "incs = [inc(i) for i in data]\n",
    "doubles = [double(i) for i in incs]\n",
    "total = sum(doubles)\n",
    "\n",
    "# Execute the computation graph\n",
    "print(\"Result of task scheduling with Dask:\", total.compute())\n",
    "\n",
    "# Distributed data structures with Dask dataframes\n",
    "df = pd.DataFrame({'x': np.random.random(100000), 'y': np.random.random(100000)})\n",
    "ddf = dd.from_pandas(df, npartitions=4)  # Create a Dask dataframe from Pandas dataframe\n",
    "mean_x = ddf['x'].mean()\n",
    "print(\"Mean of column 'x':\", mean_x.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8473b5-adfe-42ca-b879-daa04ca7cc4d",
   "metadata": {},
   "source": [
    "## 5. Describe Cassandra's features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d253",
   "metadata": {},
   "source": [
    "Apache Cassandra is a distributed NoSQL database known for its high availability, scalability, and fault tolerance. Its features include:\n",
    "\n",
    "Distributed Architecture: Cassandra is designed to operate across a cluster of nodes, where data is distributed across multiple machines. This distributed architecture ensures high availability and fault tolerance, as there's no single point of failure.\n",
    "\n",
    "Linear Scalability: Cassandra's architecture allows it to scale linearly by adding more nodes to the cluster. As the size of the cluster increases, the database's capacity and throughput also increase proportionally, making it suitable for large-scale deployments.\n",
    "\n",
    "High Availability: Cassandra provides high availability by replicating data across multiple nodes in the cluster. Data is replicated synchronously or asynchronously to ensure that even if some nodes fail, the data remains accessible and the system continues to operate.\n",
    "\n",
    "Tunable Consistency: Cassandra offers tunable consistency, allowing users to configure the consistency level on a per-operation basis. This means you can choose between strong consistency for critical operations or eventual consistency for better performance and availability.\n",
    "\n",
    "Fault Tolerance: Cassandra is designed to withstand node failures gracefully. It uses a decentralized architecture with no single point of failure, and data is replicated across multiple nodes. In case of node failures, Cassandra can automatically detect and recover from failures without impacting the overall system.\n",
    "\n",
    "Schema Flexibility: Unlike traditional relational databases, Cassandra offers schema flexibility, allowing you to store structured, semi-structured, and unstructured data without predefined schemas. This flexibility makes it well-suited for handling varied and evolving data types.\n",
    "\n",
    "Wide Column Store: Cassandra is based on a wide column store data model, also known as a column-family database. This model allows for efficient storage and retrieval of large volumes of data with fast read and write performance, particularly for use cases involving time-series data or IoT applications.\n",
    "\n",
    "Built-in Replication: Cassandra provides built-in replication capabilities, allowing you to replicate data across multiple data centers and geographic regions for disaster recovery, data locality, and improved latency. Replication strategies can be customized based on application requirements.\n",
    "\n",
    "Support for ACID Transactions: While Cassandra is primarily an eventually consistent database, it also supports lightweight transactions and atomicity, consistency, isolation, and durability (ACID) properties for specific use cases requiring stronger consistency guarantees.\n",
    "\n",
    "Overall, Cassandra's features make it a robust choice for distributed, highly available, and scalable applications, particularly in scenarios where data volume and performance requirements are high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
